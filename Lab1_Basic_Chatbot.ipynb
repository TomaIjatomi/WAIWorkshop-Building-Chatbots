{"cells":[{"cell_type":"markdown","id":"c6050b7d","metadata":{"id":"c6050b7d"},"source":["\n","# Workshop Notebook 1: LLM Fundamentals & Building a Multimodal Chatbot\n","\n","Welcome to the first part of our workshop! The goal of this notebook is to understand and practice the fundamental building blocks of Large Language Model (LLM) applications. We'll start simple and build our way up to a fun, interactive AI application: **The Creative Travel Planner** with optional **image** generation and **audio** responses.\n","\n","---\n","\n","### What you'll learn\n","1. Core LLM concepts (tokens, parameters, prompts)  \n","2. Making your **first Gemini API call** with `generate_content`  \n","3. Tuning **parameters** (`temperature`, `top_p`, `max_output_tokens`)  \n","4. Designing **system prompts / personas**  \n","5. **Streaming** responses (token‚Äëby‚Äëtoken)  \n","6. Adding simple **conversation memory**  \n","7. Building a **Travel Planner** (text ‚Üí optional image ‚Üí audio)\n","\n","We use **Gemini 2.0 Flash**, a fast, multimodal model.\n","\n","---\n","\n","> **Docs & links:**\n","> - Gemini Quickstart ‚Üí https://ai.google.dev/gemini-api/docs/quickstart  \n","> - Text generation ‚Üí https://ai.google.dev/gemini-api/docs/text-generation  \n","> - Image generation ‚Üí https://ai.google.dev/gemini-api/docs/image-generation  \n","> - Safety guidance ‚Üí https://ai.google.dev/gemini-api/docs/safety-guidance   \n","> - Gradio Chat guide ‚Üí https://www.gradio.app/guides/creating-a-chatbot-fast\n"]},{"cell_type":"markdown","id":"dc0aa9ce","metadata":{"id":"dc0aa9ce"},"source":["## 0) Setup & Install\n","First, we need to install the necessary Python libraries.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"eb537e77","metadata":{"id":"eb537e77"},"outputs":[],"source":["# Install required libraries\n","!pip -q install google-genai gradio gTTS pillow\n","\n"]},{"cell_type":"markdown","id":"01753fb3","metadata":{"id":"01753fb3"},"source":["\n","## 1) API Key Configuration\n","\n","To use the Gemini API, you need an API key.\n","\n","1. Go to [Google AI Studio](https://aistudio.google.com/) and create an API key.\n","2. In Colab, click the Key icon (Secrets) in the left sidebar.\n","3. Create a new secret named `GEMINI_API_KEY` and paste your key.\n","4. Or set an environment variable in a cell: `os.environ['GEMINI_API_KEY'] = 'YOUR_KEY'`."]},{"cell_type":"code","execution_count":null,"id":"31c21264","metadata":{"id":"31c21264"},"outputs":[],"source":["#import libraries\n","import os, getpass\n","from google import genai\n","\n","GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\") or getpass.getpass(\"üîë Enter your GEMINI_API_KEY: \")\n","#genai.configure(api_key=GEMINI_API_KEY)\n","\n","MODEL_ID = \"gemini-2.0-flash\"  # fast, multimodal\n","print(\"Gemini configured!\")\n"]},{"cell_type":"markdown","id":"54bc3818","metadata":{"id":"54bc3818"},"source":["\n","## 2) LLM Basics (Quick Overview)\n","\n","Before we start building, let‚Äôs understand a few key concepts that influence how LLMs generate text.\n","\n","In this notebook we are using Gemini 2.0 Flash, part of Google‚Äôs Gemini family of generative AI models.\n","\n","\n","For more technical details, see Google‚Äôs official documentation:\n"," [Gemini 2.0 Flash ‚Äì Model Overview](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash)\n","\n","| Parameter | Description | Typical Range / Default |\n","|------------|--------------|--------------------------|\n","| **temperature** | Controls creativity/randomness in responses. Lower = more focused; higher = more diverse. | `0.0 ‚Äì 2.0` (default ‚âà `1.0`) |\n","| **top_p** | Considers only tokens within cumulative probability *p*. | `0.1 ‚Äì 1.0` (default ‚âà `0.95`) |\n","| **top_k** | Considers only the top *k* most likely tokens. | default: `64 (fixed)`|\n","| **max_output_tokens** | Maximum number of tokens to generate. | default = `8,192`|\n","| **seed** | (Optional) Makes responses reproducible. | Integer value |\n","\n","> **Tip:** Increase `temperature` for creativity; lower it for accuracy.  \n","> Use `top_p` or `top_k` to balance diversity and focus.\n","\n","**Reference:** [Adjusting Parameter Values ‚Äî Vertex AI Docs](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)\n","\n"]},{"cell_type":"markdown","id":"abbf2df5","metadata":{"id":"abbf2df5"},"source":["## 3) Your first Gemini call"]},{"cell_type":"code","source":["# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n","client = genai.Client(api_key=GEMINI_API_KEY)\n","\n","response = client.models.generate_content(\n","    model=MODEL_ID,\n","    contents=\"Hello Gemini! What can you do?\"\n",")\n","print(response.text)"],"metadata":{"id":"IL1P5QYgRiQZ"},"id":"IL1P5QYgRiQZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Display Markdown\n","from IPython.display import Markdown, display\n","\n","display(Markdown(response.text))"],"metadata":{"id":"C8Fkox_7eBMn"},"id":"C8Fkox_7eBMn","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"e25ed384","metadata":{"id":"e25ed384"},"source":["\n","## 4) Demo #1 ‚Äî Minimal chatbot (text only)\n"]},{"cell_type":"code","source":["def chat_minimal(user_text):\n","    response = client.models.generate_content(\n","        model=MODEL_ID,\n","        contents=user_text\n","    )\n","    return response.text\n"],"metadata":{"id":"P1S7C1rUgP4S"},"id":"P1S7C1rUgP4S","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","\n","with gr.Blocks(theme=\"soft\") as demo1:\n","    gr.Markdown(\"### Minimal Chat (Gemini API Client)\")\n","    inp = gr.Textbox(label=\"Ask anything\")\n","    out = gr.Textbox(label=\"Response\", lines=6)\n","    gr.Button(\"Send\").click(chat_minimal, inp, out)\n","\n","\n"],"metadata":{"id":"_6_9IAJJZjDn"},"id":"_6_9IAJJZjDn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo1.launch()"],"metadata":{"id":"vwyfAXFVUxOk"},"id":"vwyfAXFVUxOk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo1.close()"],"metadata":{"id":"qkEnAtRuWqax"},"id":"qkEnAtRuWqax","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8e7bd823","metadata":{"id":"8e7bd823"},"source":["\n","## 5) Parameters Playground\n"]},{"cell_type":"code","source":["def generate_with_params(prompt, temperature, top_p, max_tokens):\n","    response = client.models.generate_content(\n","        model=\"gemini-2.0-flash\",\n","        contents=prompt,\n","        config={\n","            \"temperature\": float(temperature),\n","            \"top_p\": float(top_p),\n","            \"max_output_tokens\": int(max_tokens),\n","        },\n","    )\n","    return response.text.strip()"],"metadata":{"id":"MZDqi_2cbf-Y"},"id":"MZDqi_2cbf-Y","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# More creative and free-flowing\n","print(generate_with_params(\"Generate a marketing slogan for a new coffee shop in Dublin.\", temperature=2, top_p=1, max_tokens=500))\n","\n","# Balanced, informative\n","print(\"\\n-----------------------------------\")\n","print(generate_with_params(\"Generate a marketing slogan for a new coffee shop in Dublin..\", temperature=0.7, top_p=1, max_tokens=200))\n","\n","# Precise and conservative\n","print(\"\\n-----------------------------------\")\n","print(generate_with_params(\"Generate a marketing slogan for a new coffee shop in Dublin..\", temperature=0.3, top_p=1, max_tokens=100))\n"],"metadata":{"id":"3IvkxcTsdN9s"},"id":"3IvkxcTsdN9s","execution_count":null,"outputs":[]},{"cell_type":"code","source":["with gr.Blocks(theme=\"soft\") as demo2:\n","    gr.Markdown(\"### üéõÔ∏è Parameters Playground\")\n","\n","    prompt = gr.Textbox(\n","        value=\"Give me 3 creative day-trip ideas near Lisbon.\",\n","        lines=2,\n","        label=\"Prompt\"\n","    )\n","\n","    with gr.Row():\n","        temperature = gr.Slider(0.0, 2.0, 1.2, step=0.1, label=\"temperature\")\n","        top_p = gr.Slider(0.1, 1.0, 0.9, step=0.05, label=\"top_p\")\n","        max_tokens = gr.Slider(64, 512, 300, step=8, label=\"max_output_tokens\")\n","\n","    out = gr.Textbox(lines=10, label=\"Response\")\n","\n","    gr.Button(\"Generate\").click(\n","        generate_with_params,\n","        inputs=[prompt, temperature, top_p, max_tokens],\n","        outputs=out\n","    )\n","\n","#demo2.launch(share=True, inline=False)\n"],"metadata":{"id":"p7KQvtQle-T_"},"id":"p7KQvtQle-T_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo2.launch()"],"metadata":{"id":"SMWZHANQeVCw"},"id":"SMWZHANQeVCw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo2.close()\n"],"metadata":{"id":"-X5K-TnifeDB"},"id":"-X5K-TnifeDB","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"883d8560","metadata":{"id":"883d8560"},"source":["\n","## 6) System prompts (personas)\n"]},{"cell_type":"markdown","source":["In this section, we'll explore how **system prompts** (or **personas**) influence how a language model responds.  \n","A *system prompt* sets the *behavior, tone, and role* of the model ‚Äî essentially telling it **how** to respond, not just **what** to respond to.\n","\n","####  How Chat Models Process Messages\n","Most modern LLM APIs (like OpenAI and Gemini) use a **multi-message format** that mirrors a conversation.  \n","Each message has a *role* that helps the model interpret context correctly:\n","\n","| Role | Description | Example |\n","|------|--------------|----------|\n","| **system** | Sets the model‚Äôs overall behavior or persona | ‚ÄúYou are a friendly tutor who explains concepts clearly.‚Äù |\n","| **user** | Contains the user‚Äôs prompt or question | ‚ÄúExplain why the sky is blue.‚Äù |\n","| **assistant** | Holds the model‚Äôs previous response (for ongoing chats) | ‚ÄúThe sky appears blue because of Rayleigh scattering‚Ä¶‚Äù |\n","\n","> In our case, we manually include the **system** and **user** roles inside a single text prompt.  \n","> This is enough for simple, single-turn chats, but for multi-turn conversation, the Gemini API also supports structured message objects.\n","\n","---\n","\n","#### Prompt Inspiration & Examples\n","\n","If you‚Äôd like to explore examples of well-designed prompts, check out:\n","- [Google Cloud Vertex AI Prompt Gallery](https://cloud.google.com/vertex-ai/generative-ai/docs/prompt-gallery)  \n","- [Gemini Prompt Design Guide](https://ai.google.dev/gemini-api/docs/prompting)  \n","- [OpenAI Prompt Examples](https://platform.openai.com/docs/examples)  \n","\n","Further Reading - [System Instructions - Vertex AI docs](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions)\n","\n","---\n","\n","Next, we‚Äôll build an interactive **Persona Playground** in Gradio that lets you select a system prompt (persona), adjust the model‚Äôs creativity, and observe how the tone of the AI‚Äôs response changes in real time.\n"],"metadata":{"id":"zJBLFhnckI_w"},"id":"zJBLFhnckI_w"},{"cell_type":"code","source":["# Define a few example personas\n","PERSONAS = {\n","    \"Friendly Tutor\": \"You are a friendly tutor who explains concepts simply with small examples.\",\n","    \"Concise Analyst\": \"You are a precise analyst who answers briefly with bullet points.\",\n","    \"Creative Storyteller\": \"You are a playful storyteller who responds vividly and imaginatively.\",\n","}\n","\n","# Persona-based chat function\n","def persona_chat(user_text, persona, temperature):\n","    sys_prompt = PERSONAS[persona]\n","    response = client.models.generate_content(\n","        model=\"gemini-2.0-flash\",\n","        contents=f\"System: {sys_prompt}\\nUser: {user_text}\\nAI:\",\n","        config={\n","            \"temperature\": float(temperature)\n","        }\n","    )\n","    return response.text.strip()"],"metadata":{"id":"4HSxkYJ1hGw0"},"id":"4HSxkYJ1hGw0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Markdown, display\n","\n","response = persona_chat(\"Explain why the sky looks blue.\", \"Friendly Tutor\", 0.7)\n","display(Markdown(response))"],"metadata":{"id":"Vt2j3yA9h-E0"},"id":"Vt2j3yA9h-E0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Markdown, display\n","\n","response = persona_chat(\"Explain why the sky looks blue.\", \"Creative Storyteller\", 2.0)\n","display(Markdown(response))"],"metadata":{"id":"JdTKKkOpiG4f"},"id":"JdTKKkOpiG4f","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","\n","with gr.Blocks(theme=\"soft\") as demo3:\n","    gr.Markdown(\"### üé≠ Persona Playground\")\n","\n","    user_text = gr.Textbox(\n","        placeholder=\"Ask a question or give a prompt...\",\n","        label=\"Your message\",\n","        lines=2\n","    )\n","\n","    with gr.Row():\n","        persona = gr.Dropdown(\n","            choices=list(PERSONAS.keys()),\n","            value=\"Friendly Tutor\",\n","            label=\"Persona\"\n","        )\n","        temperature = gr.Slider(\n","            minimum=0.0,\n","            maximum=2.0,\n","            value=1.0,\n","            step=0.1,\n","            label=\"Temperature\"\n","        )\n","\n","    out = gr.Markdown(label=\"Response\")  # renders Markdown output\n","\n","    gr.Button(\"Generate\").click(\n","        fn=persona_chat,\n","        inputs=[user_text, persona, temperature],\n","        outputs=out\n","    )\n","\n","#demo3.launch(share=True, inline=False)\n"],"metadata":{"id":"qSLQZLBTjVfU"},"id":"qSLQZLBTjVfU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo3.launch()"],"metadata":{"id":"lK3FnPaeifqv"},"id":"lK3FnPaeifqv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo3.close()"],"metadata":{"id":"DqpNHggwjov1"},"id":"DqpNHggwjov1","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"eaaf71bf","metadata":{"id":"eaaf71bf"},"source":["\n","## 7) Streaming responses\n"]},{"cell_type":"markdown","source":["You can choose whether the model generates streaming responses or non-streaming responses. For streaming responses, you receive each response as soon as its output token is generated. For non-streaming responses, you receive all responses after all of the output tokens are generated."],"metadata":{"id":"d1jRY-4Do212"},"id":"d1jRY-4Do212"},{"cell_type":"code","source":["def stream_to_stdout(prompt, temperature=0.7):\n","    \"\"\"\n","    Streams Gemini responses token-by-token in real time\n","\n","    \"\"\"\n","    out = []\n","    print(\"Streaming response:\\n\")\n","\n","    stream = client.models.generate_content_stream(\n","        model=\"gemini-2.0-flash\",\n","        contents=prompt,\n","        config={\"temperature\": float(temperature)},\n","    )\n","\n","    for event in stream:\n","        if event.text:\n","            print(event.text, end=\"\", flush=True)\n","            out.append(event.text)\n","\n","    print()  # newline for formatting\n","    return \"\".join(out)\n","\n","# Test it\n","_ = stream_to_stdout(\"Give me 5 carry-on packing tips for a 2-day work trip.\")\n"],"metadata":{"id":"4CbDZRQUqmmN"},"id":"4CbDZRQUqmmN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8eebaa8d","metadata":{"id":"8eebaa8d"},"source":["\n","## 8) Simple memory chat\n"]},{"cell_type":"markdown","source":["LLMs like Gemini are **stateless by default** ‚Äî meaning they don‚Äôt remember anything from previous messages.  \n","Each API call is processed independently, so if you ask a follow-up question without repeating the context,  \n","the model won‚Äôt know what you‚Äôre referring to.\n","\n","To make a chatbot feel *conversational*, we can simulate memory by:\n","1. Keeping a **history** of user and AI messages.\n","2. Sending that history back to the model as part of the prompt each time.\n","\n","In this section, we‚Äôll add lightweight ‚Äúshort-term memory‚Äù using a `history` list.\n","\n","**Reference:**  \n","- [Gemini API: Text Generation](https://ai.google.dev/gemini-api/docs/text-generation)  \n","\n"],"metadata":{"id":"inrNpTskt1Tu"},"id":"inrNpTskt1Tu"},{"cell_type":"code","source":["# First call: introduce yourself\n","resp1 = client.models.generate_content(\n","    model=\"gemini-2.0-flash\",\n","    contents=\"Hello! My name is ------.\"\n",")\n","print(\"Response 1:\", resp1.text)\n","\n","# Second call: ask a follow-up without context\n","resp2 = client.models.generate_content(\n","    model=\"gemini-2.0-flash\",\n","    contents=\"What's my name?\"\n",")\n","print(\"\\nResponse 2:\", resp2.text)\n"],"metadata":{"id":"H-EY0DgbuhtP"},"id":"H-EY0DgbuhtP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def chat_with_memory(user_text, history,\n","                     system_prompt=\"You are a concise, friendly assistant.\",\n","                     max_turns=6, temperature=0.7):\n","    \"\"\"\n","    Builds lightweight conversational memory by replaying\n","    recent turns as context for the model.\n","    \"\"\"\n","    # Keep only the last few turns to stay within token limits\n","    turns = history[-max_turns:] if history else []\n","\n","    # Construct conversation transcript\n","    transcript = [f\"System: {system_prompt}\"]\n","    for user, ai in turns:\n","        transcript += [f\"User: {user}\", f\"AI: {ai}\"]\n","    transcript += [f\"User: {user_text}\", \"AI:\"]\n","\n","    # Generate a response\n","    resp = client.models.generate_content(\n","        model=\"gemini-2.0-flash\",\n","        contents=\"\\n\".join(transcript),\n","        config={\"temperature\": float(temperature)},\n","    )\n","    return resp.text.strip()\n"],"metadata":{"id":"YLvpaI2vuhpu"},"id":"YLvpaI2vuhpu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["SYSTEM_PROMPT = \"You are a friendly assistant who remembers recent details.\"\n","\n","with gr.Blocks(theme=\"soft\") as demo_memory:\n","    gr.Markdown(\"### Memory Chat (Lightweight Demo)\")\n","\n","    chatbox = gr.Chatbot(height=320)\n","    msg = gr.Textbox(label=\"Say something\", placeholder=\"e.g., My name is ----. Remember it.\")\n","    temperature = gr.Slider(0.0, 1.0, 0.7, step=0.05, label=\"Temperature\")\n","    state = gr.State([])\n","\n","    def on_submit(user_text, temp, st):\n","        st = st or []\n","        ai = chat_with_memory(user_text, st, SYSTEM_PROMPT, temperature=temp)\n","        st.append((user_text, ai))\n","        return st, st\n","\n","    msg.submit(on_submit, [msg, temperature, state], [chatbox, state])\n","\n","# Uncomment to run:\n","# demo_memory.launch(share=True, inline=False)\n"],"metadata":{"id":"bMZafSqLuhmd"},"id":"bMZafSqLuhmd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo_memory.launch()"],"metadata":{"id":"blxaB8qeuzOK"},"id":"blxaB8qeuzOK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo_memory.close()"],"metadata":{"id":"VJqGcLRovJc7"},"id":"VJqGcLRovJc7","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## Business Applications of What You‚Äôve Learned\n","\n","So far, you‚Äôve explored **how LLMs communicate** ‚Äî from text generation to parameter tuning, system prompts and memory.\n","\n","These skills are not just technical curiosities ‚Äî they‚Äôre the foundation for **real business impact** across industries.  \n","\n","### Real-World Use Cases\n","| Domain | Example Application | Description |\n","|---------|---------------------|--------------|\n","| **Travel & Hospitality** | AI Travel Planner | Personalized trip recommendations, itinerary generation, and visual previews for destinations. |\n","| **Retail & E-Commerce** | AI Product Assistant | Product description generation, comparison tools, and conversational shopping assistants. |\n","| **Customer Support** | Chatbots with Memory | Context-aware responses that simulate human-like understanding of user history. |\n","| **Marketing & Communications** | Content Generation | Email personalization, social media copywriting, and campaign ideation. |\n","| **Education & Training** | AI Tutors | Personalized learning assistants that explain, quiz, and summarize material dynamically. |\n","\n","> Generating and refining content in this way is one of the most **common and high-impact LLM use cases** today.  \n","\n","---\n","\n","## Coming Next: Retrieval-Augmented Generation (RAG)\n","\n","In the next phase, we‚Äôll bridge what you‚Äôve learned with **data grounding** ‚Äî teaching your chatbot to *access knowledge*\n","beyond what it was trained on.\n","\n","You‚Äôll learn how to:\n","- Connect LLMs to your **own data** (PDFs, websites, company documents)  \n","- Retrieve and summarize relevant context dynamically  \n","- Reduce hallucinations by grounding responses in facts  \n","- Build a **domain-specific chatbot** that truly knows your business  \n","\n","This marks the shift from **creative generation** to **knowledge-powered intelligence** ‚Äî\n","the core of most enterprise AI systems today.\n","\n","---\n","\n","üìö **Further Reading**\n","- [Google Cloud: 1,001 real-world gen AI use cases from the world's leading organizations](https://cloud.google.com/transform/101-real-world-generative-ai-use-cases-from-industry-leaders)\n","\n","---\n","\n","### üåü Reflection\n","\n","> Think of a process in your own field or company that involves **text, images, or decisions**.  \n","> Could an AI assistant like the one you built today help streamline it?  \n","> If so, you‚Äôve just identified your first **AI use case!**. üëè\n","\n","---\n"],"metadata":{"id":"18Ze-lDOwuCi"},"id":"18Ze-lDOwuCi"},{"cell_type":"code","source":[],"metadata":{"id":"D8gOB-q76nuW"},"id":"D8gOB-q76nuW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9) Bonus Section - Creative Travel Planner\n","\n","Now, let's combine everything we've learned‚ÄîAPI calls, system prompts, and multimodal capabilities‚Äîto build our final application.\n","\n","The **Creative Travel Planner** will:\n","1.  Take a user's travel request.\n","2.  Generate a text-based itinerary or set of ideas using a Gemini model.\n","3.  Optionally, generate a beautiful image of the destination using Stable Diffussion.\n","4.  Optionally, convert the text response into speech using Google's Text-to-Speech (gTTS) model."],"metadata":{"id":"RkdqCc1mjnzv"},"id":"RkdqCc1mjnzv"},{"cell_type":"markdown","source":["### **Image Generation Setup: Hugging Face API Token**\n","\n","To use a free and simple image generation model, we'll use Stable Diffusion through the Hugging Face API. This requires a separate, free API token.\n","\n","**Step-by-Step Guide to Get Your HF Token:**\n","1.  **Go to Hugging Face:**\n","    * Open your browser and navigate to [https://huggingface.co/](https://huggingface.co/).\n","2.  **Create an Account:**\n","    * Sign up for a free account.\n","3.  **Find Your Access Tokens:**\n","    * Click on your profile picture in the top-right corner, then go to **Settings**.\n","    * In the left-hand menu, click on **Access Tokens**.\n","4.  **Create a New Token:**\n","    * Click the \"**New token**\" button. Give it a name (e.g., \"WAI-Workshop\") and set the role to \"**read**\". Click \"**Generate a token**\".\n","5.  **Copy and Add to Colab Secrets:**\n","    * Copy the generated token.\n","    * In your Colab notebook, click the **Key icon (Secrets)** in the left sidebar.\n","    * Create a new secret named `HF_TOKEN` and paste your token as the value. Make sure \"Notebook access\" is enabled."],"metadata":{"id":"6voFwNJvrOm1"},"id":"6voFwNJvrOm1"},{"cell_type":"code","source":["from PIL import Image\n","import requests\n","import io\n","import uuid\n","from gtts import gTTS\n","from google.colab import userdata"],"metadata":{"id":"bmzjBR7OrKjh"},"id":"bmzjBR7OrKjh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Load the Hugging Face Token ---\n","try:\n","    HF_TOKEN = userdata.get('HF_TOKEN')\n","except userdata.SecretNotFoundError:\n","    print('Secret \"HF_TOKEN\" not found. Please follow the setup instructions.')\n","    HF_TOKEN = None"],"metadata":{"id":"rN2dAYkz2FCN"},"id":"rN2dAYkz2FCN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Model Name Constants ---\n","TEXT_MODEL_NAME = \"gemini-2.0-flash\"\n","IMAGE_MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n","\n","def generate_destination_image(prompt: str):\n","    \"\"\"Generates an image using the Hugging Face Inference API for Stable Diffusion XL.\"\"\"\n","    if not HF_TOKEN:\n","        print(\" Image generation failed: Hugging Face token not found.\")\n","        return None\n","    print(f\"Generating image with {IMAGE_MODEL_ID}...\")\n","    api_url = f\"https://api-inference.huggingface.co/models/{IMAGE_MODEL_ID}\"\n","    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n","    payload = {\"inputs\": f\"A beautiful, vibrant, photorealistic travel advertisement photo of {prompt}, cinematic lighting, 8k\"}\n","    try:\n","        response = requests.post(api_url, headers=headers, json=payload, timeout=90)\n","        response.raise_for_status()\n","        image_bytes = response.content\n","        return Image.open(io.BytesIO(image_bytes))\n","    except requests.exceptions.RequestException as e:\n","        print(f\" Image generation failed: {e}\")\n","        if response.status_code == 503:\n","            print(\"The model might be loading on the Hugging Face servers. Please try again shortly.\")\n","        else:\n","            print(f\"Full response: {response.text}\")\n","        return None\n"],"metadata":{"id":"-K-_G5UR1XH4"},"id":"-K-_G5UR1XH4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def synthesize_audio(text: str, directory=\"/tmp\"):\n","    \"\"\"Converts text to speech using gTTS and saves it as an MP3 file.\"\"\"\n","    print(\" Synthesizing audio...\")\n","    os.makedirs(directory, exist_ok=True)\n","    path = os.path.join(directory, f\"tts_{uuid.uuid4().hex[:8]}.mp3\")\n","    try:\n","        tts = gTTS(text)\n","        tts.save(path)\n","        return path\n","    except Exception as e:\n","        print(f\" Audio synthesis failed: {e}\")\n","        return None"],"metadata":{"id":"-QlRnlDy1W1J"},"id":"-QlRnlDy1W1J","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","TRAVEL_SYSTEM_PROMPT = \"\"\"You are a creative and enthusiastic travel planner.\n","Your goal is to provide exciting and practical travel ideas.\n","Be concise and use bullet points for itineraries.\n","IMPORTANT: Do not use any markdown formatting. Do not use asterisks, hashes, or any other special characters. Respond in plain text only.\n","\"\"\"\n","\n","def plan_my_trip(user_request, generate_image_flag, generate_audio_flag):\n","    \"\"\"Main function to handle Gradio inputs and generate multimodal outputs.\"\"\"\n","    print(f\" Processing request: '{user_request}'\")\n","\n","    # --- 1. Generate Text Response ---\n","    full_prompt = f\"System: {TRAVEL_SYSTEM_PROMPT}\\nUser: {user_request}\\nAI:\"\n","    response = client.models.generate_content(\n","        model=TEXT_MODEL_NAME,\n","        contents=full_prompt\n","    )\n","    # This single, clean text variable is used for both display and audio\n","    plain_text_response = response.text.strip()\n","\n","    # --- 2. Generate Image (if requested) ---\n","    image_output = None\n","    if generate_image_flag:\n","        image_output = generate_destination_image(user_request)\n","\n","    # --- 3. Generate Audio (if requested) ---\n","    audio_output_path = None\n","    if generate_audio_flag:\n","        # We can now directly use the clean text from the model\n","        audio_output_path = synthesize_audio(plain_text_response)\n","\n","    return plain_text_response, image_output, audio_output_path"],"metadata":{"id":"6sD0oJi5rKf3"},"id":"6sD0oJi5rKf3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","\n","with gr.Blocks(theme=gr.themes.Soft()) as demo_travel:\n","    gr.Markdown(\"# üåç Creative Travel Planner\")\n","    gr.Markdown(\"Describe your dream trip, and let the AI assistant plan it for you!\")\n","\n","    with gr.Row():\n","        with gr.Column(scale=3):\n","            user_input = gr.Textbox(\n","                label=\"Your Trip Request\",\n","                placeholder=\"e.g., Plan a 3-day romantic weekend in Rome\",\n","                lines=2\n","            )\n","            with gr.Row():\n","                want_image = gr.Checkbox(label=\"üñºÔ∏è Generate an Image\", value=True)\n","                want_audio = gr.Checkbox(label=\"üó£Ô∏è Generate Audio\", value=False)\n","\n","            submit_button = gr.Button(\"Plan my Trip\", variant=\"primary\")\n","\n","        with gr.Column(scale=2):\n","            plan_output_text = gr.Textbox(label=\"Itinerary & Suggestions\", lines=10)\n","            plan_output_image = gr.Image(type=\"pil\", label=\"Destination Visual\")\n","            plan_output_audio = gr.Audio(label=\"Spoken Itinerary\", autoplay=False)\n","\n","    gr.Examples(\n","        examples=[\n","            [\"A 2-day budget-friendly trip to Lisbon\", True, False],\n","            [\"Suggest a relaxing beach holiday in Thailand\", True, True],\n","            [\"What are the best things to do on a family trip to Dublin?\", False, False],\n","        ],\n","        inputs=[user_input, want_image, want_audio]\n","    )\n","\n","    submit_button.click(\n","        fn=plan_my_trip,\n","        inputs=[user_input, want_image, want_audio],\n","        outputs=[plan_output_text, plan_output_image, plan_output_audio]\n","    )\n","\n","print(\"Gradio app ready. Launching...\")\n","demo_travel.launch(debug=True)"],"metadata":{"id":"HBKCAFO4rKvP"},"id":"HBKCAFO4rKvP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo_travel.close()"],"metadata":{"id":"jOs-rgZG2ghg"},"id":"jOs-rgZG2ghg","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oNcBfsqNpLVz"},"id":"oNcBfsqNpLVz","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}