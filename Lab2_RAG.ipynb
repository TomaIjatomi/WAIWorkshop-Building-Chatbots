{"cells":[{"cell_type":"markdown","metadata":{"id":"Cz6iWX9Q2lr2"},"source":["# Lab 2 ‚Äî Retrieval-Augmented Generation (RAG) with LangChain & Gemini\n","\n","Welcome to the second part of our workshop! In Lab 1, we learned how to generate creative text. Now, we'll solve a key limitation: how to make an LLM answer questions about specific, private, or very recent data it wasn't trained on.\n","\n","We'll build the **EU AI News Navigator**, a Q&A application that uses RAG to answer questions about recent EU AI Act news.\n","\n","**What you'll learn**\n","### What you'll learn\n","1.  **The \"Why\":** Understand the limitations of standard LLMs and why RAG is necessary.\n","2.  **The Core Components of RAG:** Learn about Document Loading, Text Splitting (Chunking), Embeddings, and Vector Stores.\n","3.  **Building a RAG Chain with LangChain:** Use LangChain to quickly assemble a Q&A pipeline.\n","4.  Build a Gradio app that answers questions based on our custom, up-to-date documents.\n","\n","\n","**References**\n","- LangChain: [Question Answering / Retrieval](https://python.langchain.com/docs/use_cases/question_answering/)\n","- Google Gemini API: [Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"92zt9OqY2lr3"},"source":["## ‚öôÔ∏è 1) Setup & API Key\n","\n","First, we'll install the necessary libraries and configure your Gemini API key.\n"]},{"cell_type":"code","source":["!pip install -q langchain langchain_community langchain-google-genai faiss-cpu unstructured gradio 2>/dev/null"],"metadata":{"id":"qZpOU2eEt7j4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnvoQkIW2lr4"},"outputs":[],"source":["import os\n","import getpass\n","import textwrap\n","from IPython.display import display, Markdown\n","\n","# Configure your Gemini API Key.\n","if 'GEMINI_API_KEY' not in os.environ:\n","    os.environ['GEMINI_API_KEY'] = getpass.getpass(\"Enter your GEMINI_API_KEY: \")\n","\n","# # Some LangChain integrations historically look for GOOGLE_API_KEY.\n","# This alias ensures compatibility by setting it if GEMINI_API_KEY is present.\n","if 'GOOGLE_API_KEY' not in os.environ and 'GEMINI_API_KEY' in os.environ:\n","    os.environ['GOOGLE_API_KEY'] = os.environ['GEMINI_API_KEY']\n","\n","print(\"GEMINI_API_KEY detected:\", \"Yes\" if os.environ.get(\"GEMINI_API_KEY\") else \"No\")\n"]},{"cell_type":"markdown","metadata":{"id":"7M34mtBFq0GK"},"source":["## 2) The Problem: An LLM's Knowledge is Limited\n","\n","Let's ask a standard Gemini model a specific question about the brand-new initiatives and tools the European Commission launched just last week (October 8, 2025) to help businesses and researchers comply with the EU AI Act. This information was published just a few days ago, so the model has no pre-trained knowledge of it.\n","\n","**Source -** European Commission News: [Commission launches AI Act Service Desk and Single Information Platform](https://digital-strategy.ec.europa.eu/en/news/commission-launches-ai-act-service-desk-and-single-information-platform-support-ai-act)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YwH5r6r52lr5"},"outputs":[],"source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n","question = \"I'm a startup owner in Dublin. What new resources did the EU launch in October 2025 to help me understand the AI Act?\"\n","\n","print(\"--- Asking the standard LLM (without RAG) ---\")\n","response = llm.invoke(question)\n","display(Markdown(response.content))\n"]},{"cell_type":"markdown","metadata":{"id":"ColJAXnx2lr5"},"source":["As expected when an LLM doesn't know the answer to a question, it typically does one of two things: it either invents an answer or it states that it cannot answer. The response here is generic; not fact-based or specific. It doesn't know about the AI Act Service Desk or the Compliance Checker because those are brand-new.\n","\n","This is the exact problem RAG solves.\n"]},{"cell_type":"markdown","source":["\n","## 3) Load the Corpus (EU AI Act News mini‚Äëdataset)\n","In this section, we‚Äôll work with a small knowledge base built from recent EU policy updates on AI regulation.\n","The corpus simulates a real-world dataset that our Retrieval-Augmented Generation (RAG) system will later use to answer questions.\n","\n","### About the Dataset\n","\n","This mini-dataset contains text excerpts from official news sources, summarizing the European Commission‚Äôs latest AI initiatives (October 2025):\n","\n","- ‚ÄúNew Tools for AI Act Compliance‚Äù - Covers the launch of tools like the AI Act Service Desk, Single Information Platform, Compliance Checker, and AI Act Explorer.\n","\n","  Source: European Commission News: [Commission launches AI Act Service Desk and Single Information Platform](https://digital-strategy.ec.europa.eu/en/news/commission-launches-ai-act-service-desk-and-single-information-platform-support-ai-act) (Published Oct 8, 2025)\n","\n","- ‚ÄúNew EU AI Strategies for Industry and Science‚Äù - ‚Üí Describes the Apply AI Strategy and the AI in Science Strategy, including the creation of RAISE (Resource for AI Science in Europe).\n","\n","  Source: Development Aid Report: [EU launches ‚Ç¨1bn AI strategies for industry and science](https://www.developmentaid.org/news-stream/post/200924/eu-ai-strategies-apply-ai-science-strategy-european-commission) (Published Oct 12, 2025)\n","\n","### Why This Dataset?\n","\n","Ultra-Recent: Published in October 2025 ‚Äî outside most models‚Äô training windows.\n","\n","Jargon-Rich: Includes official program names and technical language ideal for retrieval.\n","\n","Realistic Scenario: Mimics how teams might use RAG to explore policy documents, compliance guidance, or legal updates.\n","\n","Next step: Let‚Äôs load these text files into our environment and preview their contents before embedding them.\n"],"metadata":{"id":"mMle3qHq6oVZ"}},{"cell_type":"code","source":["# Grant notebook access to Google Drive files\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"UzsWhJWfSNzx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Define the folder containing your corpus\n","DATA_DIR = \"/content/drive/MyDrive/WAIWorkshop - Chatbots/EU_AI_Act_News_Corpus\"\n","\n","# Check if folder exists\n","if not os.path.exists(DATA_DIR):\n","    print(\"Folder not found:\", DATA_DIR)\n","    print(\"Please upload the text files to the folder specified in the data_dir.\")\n","    os.makedirs(DATA_DIR, exist_ok=True)\n","else:\n","    print(f\"Found corpus folder: {DATA_DIR}\")\n","    files = [f for f in os.listdir(DATA_DIR) if f.endswith('.txt')]\n","    if not files:\n","        print(\"No .txt files found! Please upload your corpus files.\")\n","    else:\n","        print(f\"Found {len(files)} text files:\")\n","        for f in files:\n","            print(\"   -\", f)\n"],"metadata":{"id":"xqy8TIDYCUzW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHxWeo_w2lr5"},"source":["## 4) Build the RAG Pipeline\n","\n","Now we'll use LangChain to build a basic RAG pipeline.\n","\n","Ref: https://python.langchain.com/docs/tutorials/rag/\n","\n","\n","A typical RAG application has two main components:\n","\n","1. Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n","   - Load: First we need to load our data. This is done with Document Loaders.\n","    - Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n","    - Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.0\n","\n","2. Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n"]},{"cell_type":"code","source":["# import libraries\n","from langchain_community.document_loaders import DirectoryLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.chains import create_retrieval_chain"],"metadata":{"id":"NAWg0gmZCtfD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Documents\n","LangChain has hundreds of document loaders, with over 200 listed in its community integrations for loading data from a vast array of sources. Specific loaders are available for common formats such as CSV, JSON, and PDF, as well as integrations for services like Notion, Google Drive, and YouTube.\n","\n","See Full list of Document Loaders [here](https://python.langchain.com/docs/integrations/document_loaders/)\n","\n","We will be using the [DirectoryLoader](https://python.langchain.com/docs/how_to/document_loader_directory/) which loads files from a folder. It supports formats like PDF, HTML, Markdown and Text ."],"metadata":{"id":"5d2KSbb8lR8G"}},{"cell_type":"code","source":["# 1. Load Documents\n","loader = DirectoryLoader(DATA_DIR, glob=\"*.txt\", show_progress=True)\n","docs = loader.load()\n","print(f\"\\n Loaded {len(docs)} documents.\")"],"metadata":{"id":"jPCOkebtCtWk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(docs[1].page_content)"],"metadata":{"id":"eyiz4csWF1E5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(docs[0].page_content[:500])"],"metadata":{"id":"3tHbFRKWF8KU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Split Documents into Chunks\n","\n","Now that we have loaded our documents, we need to process them for our model. LLMs have a limited context window, meaning they can only process a certain amount of text at once. To work around this, we must break our documents into smaller pieces. This process is called chunking. The goal is to create chunks that are small enough to fit in the model's context window while retaining their semantic meaning.\n","\n","There are several ways to split text; including Character Splitting, Token Splitting, Semantic Chunking; each with its own advantages. For more\n","information, see the [Langchain Docs](https://python.langchain.com/docs/concepts/text_splitters/)\n","\n","We'll use the RecursiveCharacterTextSplitter, which is a recommended starting point. It tries to keep related text together by splitting on paragraphs, then lines, and so on."],"metadata":{"id":"a8M1MnAhoIB6"}},{"cell_type":"code","source":["# 2. Split Documents into Chunks\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","print(f\"Split into {len(splits)} chunks.\")"],"metadata":{"id":"jtwl2E6XEFL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create Embeddings and Vector Store (using FAISS)\n","\n","To perform retrieval, we need to compare a user's query with our document chunks based on their semantic meaning (semantic search).\n","\n","To enable semantic search, we must convert our text chunks into numerical representations called embeddings. These are vectors that capture the meaning of the text. We then load these vectors into a vector store, a specialized database designed for efficient similarity searching.\n","\n","See links for more information on Embedding Models and Vector Stores\n","\n","[Embedding Models](https://python.langchain.com/docs/concepts/embedding_models/)\n","\n","[Vector Stores](https://python.langchain.com/docs/concepts/vectorstores/)\n","\n","\n","### **Implementation**\n","We will use a Hugging Face model to create the embeddings and FAISS (Facebook AI Similarity Search) as our in-memory vector store.\n","\n","\n","*   [all-MiniLM-L6-v2 model](http://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) - maps sentences & paragraphs to a 384 dimensional dense vector space"],"metadata":{"id":"yiOsdIOnwfzy"}},{"cell_type":"code","source":["# 3. Create Embeddings and Vector Store (using FAISS)\n","\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","\n","# Load a compact, high-quality embedding model\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","# Create FAISS vector store\n","from langchain.vectorstores import FAISS\n","vectorstore = FAISS.from_documents(splits, embedding=embeddings)\n","\n","print(\"Vector store created successfully (using Hugging Face embeddings).\")\n"],"metadata":{"id":"NFSwlB67EE-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create the RAG Chain\n","Now, we'll assemble our components into a runnable chain. This chain will automatically handle the two-step RAG process: retrieving relevant documents and then generating an answer based on them.\n","\n","### **Implementation**\n","\n","* Define a Prompt: We create a prompt template that instructs the LLM to answer a question (input) using only the provided documents (context). This helps prevent the model from making things up.\n","\n","* Create a Retriever: We convert our vector store into a retriever, which is an object designed to fetch relevant documents.\n","\n","* Build the Chain: create_retrieval_chain links the retriever with the prompt and the LLM (llm). This single retrieval_chain object now encapsulates the entire RAG workflow."],"metadata":{"id":"q7118Oh00Yhe"}},{"cell_type":"code","source":["# 4. Create the RAG Chain\n","prompt = ChatPromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n","If you don't know the answer, just say that you don't know.\n","\n","<context>\n","{context}\n","</context>\n","\n","Question: {input}\"\"\")\n","\n","document_chain = create_stuff_documents_chain(llm, prompt)\n","retriever = vectorstore.as_retriever()\n","retrieval_chain = create_retrieval_chain(retriever, document_chain)\n","\n","print(\"RAG chain created successfully!\")"],"metadata":{"id":"OvFTygwzEsvz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_Hg8_JR2lr5"},"source":["## 5) Test the RAG Chain & Inspect Retrieval\n","\n","Let's ask our question again. We'll also inspect the documents the retriever fetched to understand how it's working.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-ZZwdez2lr6"},"outputs":[],"source":["\n","question = input(\"Enter your question (or press Enter to use default): \").strip()\n","if not question:\n","    question = \"I'm a startup owner in Dublin. What new resources did the EU launch in October 2025 to help me understand the AI Act?\"\n","print(\"--- Asking the RAG-Powered LLM ---\")\n","response = retrieval_chain.invoke({\"input\": question})\n","display(Markdown(response[\"answer\"]))\n"]},{"cell_type":"markdown","metadata":{"id":"Lrs6pL3d2lr6"},"source":["### Tune Retrieval (k) & Inspect Context\n","The `k` parameter in the retriever determines how many chunks of text are fetched from the vector store. Let's inspect what the retriever is finding.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjOEpBHr2lr6"},"outputs":[],"source":["# Set k to 3 to see the top 3 most relevant chunks\n","retriever_with_k = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n","\n","print(f\"Inspecting retrieved documents for question: '{question}'\\n\")\n","retrieved_docs = retriever_with_k.invoke(question)\n","\n","for i, doc in enumerate(retrieved_docs):\n","    print(f\"--- Document {i+1} ---\")\n","    print(f\"Source: {doc.metadata.get('source')}\")\n","    display(Markdown(textwrap.shorten(doc.page_content, width=400, placeholder=\"...\")))\n"]},{"cell_type":"markdown","metadata":{"id":"t1Ah8rg82lr6"},"source":["## 6) Build the Gradio App: \"EU AI News Navigator\"\n","\n","Finally, let's wrap our chain in a Gradio UI. A key feature is displaying the sources used for each answer, which is crucial for building trust and transparency.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKecNsE12lr6"},"outputs":[],"source":["import gradio as gr\n","\n","def get_rag_response(user_question):\n","    \"\"\"Invokes the RAG chain and returns the answer and formatted sources.\"\"\"\n","    response = retrieval_chain.invoke({\"input\": user_question})\n","\n","    answer = response.get(\"answer\", \"Sorry, I couldn't find an answer.\")\n","    sources_text = \"\\n\\n---\\n**Sources Used:**\\n\"\n","\n","    # Get unique sources from the context\n","    unique_sources = {doc.metadata.get('source', 'Unknown') for doc in response.get(\"context\", [])}\n","\n","    for source in sorted(list(unique_sources)):\n","        sources_text += f\"- {os.path.basename(source)}\\n\"\n","\n","    return answer + sources_text\n","\n","with gr.Blocks(theme=gr.themes.Soft()) as demo:\n","    gr.Markdown(\"# üá™üá∫ EU AI News Navigator\")\n","    gr.Markdown(\"Ask me any question about the new EU AI Act tools and strategies launched in October 2025. My knowledge is based on the documents provided in this workshop.\")\n","\n","    with gr.Row():\n","        inp = gr.Textbox(label=\"Your Question\", lines=2, placeholder=\"e.g., What is the 'Apply AI Alliance'?\")\n","        out = gr.Markdown(label=\"Answer from Documents\")\n","\n","    btn = gr.Button(\"Ask Navigator\", variant=\"primary\")\n","    btn.click(get_rag_response, inp, out)\n","\n","    gr.Examples(\n","        examples=[\n","            [\"What is RAISE and how much funding is it getting?\"],\n","            [\"I run a small business. What new tools can I use to understand the AI Act?\"],\n","            [\"When do the rules for high-risk AI systems fully apply?\"]\n","        ],\n","        inputs=inp\n","    )\n","\n","print(\"Gradio app ready. Launching...\")\n","demo.launch(debug=True)\n"]},{"cell_type":"code","source":["demo.close()"],"metadata":{"id":"K1xS_t0xHNWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X97wVhLv2lr6"},"source":["## Wrap‚ÄëUp\n","\n","Congratulations on building a complete Retrieval-Augmented Generation (RAG) pipeline! You've successfully implemented the core workflow that powers question-answering systems. This forms a powerful baseline for building applications that can reason about private or up-to-date information.\n","\n","### **Next Steps:**\n","- Experiment with different document loaders (e.g., `PyPDFLoader`).\n","- Persist your vector store so you don't have to rebuild it every time.\n","\n","### **Beyond the Basics: Exploring Advanced RAG**\n","\n","The RAG pipeline you built is fantastic, but the field is evolving rapidly. Here are several exciting concepts and techniques you can explore to make your RAG systems even more powerful and intelligent.\n","\n","1. Optimize the Core Pipeline\n","Before moving to complex architectures, you can significantly improve the performance of the basic RAG flow:\n","\n","    * Smarter Chunking: Instead of splitting by a fixed character count, try Semantic Chunking. This method splits text based on semantic similarity, keeping related ideas together in the same chunk, which can greatly improve context.\n","\n","    * Hybrid Search: Combine vector search (semantic) with traditional keyword search (like BM25). This is powerful for queries that depend on specific keywords or acronyms.\n","\n","    * Re-ranking: Use a two-stage process. First, retrieve a larger number of documents (e.g., 20). Then, use a more powerful, slower model (a cross-encoder) to re-rank the initial results and select the top few (e.g., 3-5) to send to the LLM.\n","\n","\n","2. Agentic RAG\n","This is the cutting edge. In a simple RAG chain, the process is static. In Agentic RAG,  you use an LLM-powered agent that can think, reason, and use tools. The agent can make decisions in a loop:\n","\n","    * Analyze the Query: The agent first looks at the user's question and decides on a plan.\n","\n","    * Decide to Retrieve (or Not): It might decide that the question is simple and doesn't require retrieval at all.\n","\n","    * Perform Complex Searches.\n","\n","    * Reflect and Refine: The agent can look at the search results and decide they aren't good enough, then reformulate the query and search again.\n","\n","Further Reading\n","* [Langchain Docts - Retrieval](https://python.langchain.com/docs/concepts/retrieval/)\n","* [DeepLearning.AI - Retrieval Augmented Generation (RAG) Course](https://www.deeplearning.ai/courses/retrieval-augmented-generation-rag/)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"QQfR98ZeIwH6"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}